{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11465406,"datasetId":7184789,"databundleVersionId":11908244}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"49be2de2-d386-4ff2-abd1-1d8461bb71d8","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass CNN(nn.Module):\n    def __init__(\n        self,\n        input_dimension: tuple,\n        number_of_filters: int,\n        filter_size: tuple,\n        stride: int,\n        padding: int,\n        max_pooling_size: tuple,\n        n_neurons: int,\n        n_classes: int,\n        conv_activation: nn.Module,\n        dense_activation: nn.Module,\n        dropout_rate: float,\n        use_batchnorm: bool,\n        factor: float,\n        dropout_organisation: int,\n    ):\n        super(CNN, self).__init__()\n\n        self.conv_blocks = nn.ModuleList()\n        in_channels = input_dimension[0]\n\n        for i in range(5):\n            out_channels = int((factor ** i) * number_of_filters)\n            out_channels = max(out_channels, 3)\n\n            add_dropout = (i % dropout_organisation) > 0\n\n            conv_block = self.create_conv_block(\n                in_channels,\n                out_channels,\n                filter_size,\n                max_pooling_size,\n                stride,\n                padding,\n                conv_activation,\n                dropout_rate,\n                use_batchnorm,\n                add_dropout,\n            )\n            self.conv_blocks.append(conv_block)\n            in_channels = out_channels\n\n        self.flatten = nn.Flatten()\n\n        # Compute the size after conv layers\n        dummy_input = torch.ones(1, *input_dimension)\n        with torch.no_grad():\n            x = dummy_input\n            for block in self.conv_blocks:\n                x = block(x)\n        in_features = x.view(1, -1).shape[1]\n\n        # Define dense layers\n        self.dense_block1 = nn.Sequential(\n            nn.Linear(in_features=in_features, out_features=n_neurons),\n            dense_activation,\n            nn.Linear(n_neurons, n_classes),\n            nn.LogSoftmax(dim=1),\n        )\n\n    def create_conv_block(\n        self,\n        in_c,\n        out_c,\n        kernel_size,\n        max_pooling_size,\n        stride,\n        padding,\n        conv_activation,\n        dropout_rate,\n        use_batchnorm,\n        add_dropout,\n    ):\n        layers = [\n            nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding),\n            conv_activation\n        ]\n        if use_batchnorm:\n            layers.append(nn.BatchNorm2d(out_c))\n        layers.append(nn.MaxPool2d(kernel_size=max_pooling_size))\n        if add_dropout:\n            layers.append(nn.Dropout(p=dropout_rate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        for block in self.conv_blocks:\n            x = block(x)\n        x = self.flatten(x)\n        return self.dense_block1(x)\n","metadata":{"id":"49be2de2-d386-4ff2-abd1-1d8461bb71d8","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:31:49.725284Z","iopub.execute_input":"2025-04-18T19:31:49.725492Z","iopub.status.idle":"2025-04-18T19:31:49.734499Z","shell.execute_reply.started":"2025-04-18T19:31:49.725475Z","shell.execute_reply":"2025-04-18T19:31:49.733863Z"}},"outputs":[],"execution_count":16},{"id":"13ab057d-8754-4312-8ec9-f4601a3dc2e0","cell_type":"code","source":"import gc\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\nimport wandb\nfrom torch.cuda.amp import GradScaler, autocast","metadata":{"id":"13ab057d-8754-4312-8ec9-f4601a3dc2e0","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:31:53.301396Z","iopub.execute_input":"2025-04-18T19:31:53.301702Z","iopub.status.idle":"2025-04-18T19:31:53.306352Z","shell.execute_reply.started":"2025-04-18T19:31:53.301676Z","shell.execute_reply":"2025-04-18T19:31:53.305549Z"}},"outputs":[],"execution_count":17},{"id":"boGW01doTIRd","cell_type":"code","source":"# Device setup\ndevice= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"boGW01doTIRd","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:31:54.202020Z","iopub.execute_input":"2025-04-18T19:31:54.202815Z","iopub.status.idle":"2025-04-18T19:31:54.206362Z","shell.execute_reply.started":"2025-04-18T19:31:54.202790Z","shell.execute_reply":"2025-04-18T19:31:54.205589Z"}},"outputs":[],"execution_count":18},{"id":"Xv1-XGTzTOZs","cell_type":"code","source":"# Dataset path\ntraining_data_path = \"/kaggle/input/nature-12k/inaturalist_12K/train\"","metadata":{"id":"Xv1-XGTzTOZs","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:31:55.113999Z","iopub.execute_input":"2025-04-18T19:31:55.114756Z","iopub.status.idle":"2025-04-18T19:31:55.118015Z","shell.execute_reply.started":"2025-04-18T19:31:55.114730Z","shell.execute_reply":"2025-04-18T19:31:55.117254Z"}},"outputs":[],"execution_count":19},{"id":"04f5380d-de25-425d-bd42-f85b54a333c8","cell_type":"code","source":"torch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:31:59.271617Z","iopub.execute_input":"2025-04-18T19:31:59.271888Z","iopub.status.idle":"2025-04-18T19:31:59.275554Z","shell.execute_reply.started":"2025-04-18T19:31:59.271868Z","shell.execute_reply":"2025-04-18T19:31:59.274831Z"}},"outputs":[],"execution_count":20},{"id":"y87yZm7PTR8e","cell_type":"code","source":"# Fixed hyperparameters\nconfig = {\n    'number_of_filters': 64,\n    'filter_size': 3,\n    'stride': 1,\n    'padding': 1,\n    'max_pooling_size': 2,\n    'n_neurons': 256,\n    'n_classes': 10,\n    'conv_activation': 'relu',\n    'dense_activation': 'relu',\n    'dropout_rate': 0.3,\n    'use_batchnorm': True,\n    'factor': 2,\n    'learning_rate': 1e-3,\n    'batch_size': 16,\n    'epochs': 10,\n    'use_augmentation': True,\n    'dropout_organisation': 3\n}","metadata":{"id":"y87yZm7PTR8e","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:40:43.235308Z","iopub.execute_input":"2025-04-18T19:40:43.236035Z","iopub.status.idle":"2025-04-18T19:40:43.240033Z","shell.execute_reply.started":"2025-04-18T19:40:43.236009Z","shell.execute_reply":"2025-04-18T19:40:43.239244Z"}},"outputs":[],"execution_count":31},{"id":"fGHjJ7LeTkC6","cell_type":"code","source":"def get_transform(use_augmentation):\n    if use_augmentation:\n        return transforms.Compose([\n            transforms.RandomCrop(50, padding=1),\n            transforms.RandomGrayscale(p=0.1),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(degrees=(0, 20)),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n            )\n        ])\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        )\n    ])","metadata":{"id":"fGHjJ7LeTkC6","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:32:04.471662Z","iopub.execute_input":"2025-04-18T19:32:04.472419Z","iopub.status.idle":"2025-04-18T19:32:04.478956Z","shell.execute_reply.started":"2025-04-18T19:32:04.472386Z","shell.execute_reply":"2025-04-18T19:32:04.478247Z"}},"outputs":[],"execution_count":22},{"id":"a1bf80f5-d50a-4435-9678-23488ed9a7fc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"561f6b7c-bfe3-4b50-84f6-e8e564b1b6ad","cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'name':'PART_A_Q2_SWEEP_1',\n    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n    'parameters': {\n        'number_of_filters': {'values': [16, 32, 64, 128, 256]},\n        'filter_size': {'value': 3},\n        'stride': {'value': 1},\n        'padding': {'value': 1},\n        'max_pooling_size': {'value': 2},\n        'n_neurons': {'values': [64, 128, 256, 512, 1024]},\n        'n_classes': {'value': 10},\n        'conv_activation': {'values': ['relu', 'gelu', 'silu', 'mish', 'relu6', 'tanh', 'sigmoid']},\n        'dense_activation': {'values': ['relu', 'gelu', 'silu', 'mish', 'relu6', 'tanh', 'sigmoid']},\n        'dropout_rate': {'values': [0.2, 0.3, 0.4, 0.5]},\n        'use_batchnorm': {'values': [True, False]},\n        'factor': {'values': [1, 2, 3, 0.5]},\n        'learning_rate': {'values': [1e-2, 1e-3, 1e-4, 1e-5]},\n        'batch_size': {'value': 16},\n        'epochs': {'values': [5, 10, 15]},\n        'use_augmentation': {'values': [True, False]},\n        'dropout_organisation': {'values': [1, 2, 3, 4, 5]},\n    },\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:39:57.874267Z","iopub.execute_input":"2025-04-18T19:39:57.874543Z","iopub.status.idle":"2025-04-18T19:39:57.880507Z","shell.execute_reply.started":"2025-04-18T19:39:57.874525Z","shell.execute_reply":"2025-04-18T19:39:57.879768Z"}},"outputs":[],"execution_count":30},{"id":"ALVSOXgATqYh","cell_type":"code","source":"def train_model():\n    # Dataset\n    dataset = ImageFolder(root=training_data_path, transform=get_transform(config['use_augmentation']))\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_set, val_set = random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n\n    # Activation options\n    activations = {\n        'relu': nn.ReLU(),\n        'gelu': nn.GELU(),\n        'silu': nn.SiLU(),\n        'mish': nn.Mish(),\n        'relu6': nn.ReLU6(),\n        'tanh': nn.Tanh(),\n        'sigmoid': nn.Sigmoid(),\n    }\n\n    # Model\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = CNN(\n        input_dimension=(3, 224, 224),\n        number_of_filters=config['number_of_filters'],\n        filter_size=(config['filter_size'], config['filter_size']),\n        stride=config['stride'],\n        padding=config['padding'],\n        max_pooling_size=(config['max_pooling_size'], config['max_pooling_size']),\n        n_neurons=config['n_neurons'],\n        n_classes=config['n_classes'],\n        conv_activation=activations[config['conv_activation']],\n        dense_activation=activations[config['dense_activation']],\n        dropout_rate=config['dropout_rate'],\n        use_batchnorm=config['use_batchnorm'],\n        factor=config['factor'],\n        dropout_organisation=config['dropout_organisation'],\n    ).to(device)\n\n    # Optimizer and loss\n    optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.CrossEntropyLoss()\n    scaler = GradScaler()\n\n    # Training\n    for epoch in range(config[\"epochs\"]):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for x, y in train_loader:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad()\n            \n            # Mixed precision training\n            with autocast():\n                pred = model(x)\n                loss = criterion(pred, y)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * x.size(0)\n            correct += (pred.argmax(1) == y).sum().item()\n            total += y.size(0)\n            del x, y\n\n        train_accuracy = 100 * correct / total\n        avg_train_loss = train_loss / total\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                pred = model(x)\n                loss = criterion(pred, y)\n                val_loss += loss.item() * x.size(0)\n                correct += (pred.argmax(1) == y).sum().item()\n                total += y.size(0)\n                del x, y\n\n        val_accuracy = 100 * correct / total\n        avg_val_loss = val_loss / total\n\n        print(f\"Epoch [{epoch+1}/{config['epochs']}]\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n        print(\"-\" * 50)\n\n    # Save the trained model\n    save_path = \"/kaggle/input/nature-12k/inaturalist_12K\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Model saved to {save_path}\")\n    \n    return model","metadata":{"id":"ALVSOXgATqYh","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:32:21.802179Z","iopub.execute_input":"2025-04-18T19:32:21.802835Z","iopub.status.idle":"2025-04-18T19:32:21.813761Z","shell.execute_reply.started":"2025-04-18T19:32:21.802812Z","shell.execute_reply":"2025-04-18T19:32:21.813034Z"}},"outputs":[],"execution_count":24},{"id":"c506b49e-4e1f-4e83-ad67-a85719204b2b","cell_type":"code","source":"def wandb_sweep():\n    with wandb.init() as run:\n        sweep_config = dict(wandb.config)\n        dataset = ImageFolder(root=training_data_path, transform=get_transform(sweep_config['use_augmentation']))\n        train_size = int(0.8 * len(dataset))\n        val_size = len(dataset) - train_size\n        train_set, val_set = random_split(dataset, [train_size, val_size])\n        \n        # Optimized DataLoaders\n        train_loader = DataLoader(\n            train_set, \n            batch_size=sweep_config['batch_size'], \n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        val_loader = DataLoader(\n            val_set, \n            batch_size=sweep_config['batch_size'], \n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n\n        activations = {\n            'relu': nn.ReLU(),\n            'gelu': nn.GELU(),\n            'silu': nn.SiLU(),\n            'mish': nn.Mish(),\n            'relu6': nn.ReLU6(),\n            'tanh': nn.Tanh(),\n            'sigmoid': nn.Sigmoid(),\n        }\n        gc.collect()\n        torch.cuda.empty_cache()\n        model = CNN(\n            input_dimension=(3, 224, 224),\n            number_of_filters=sweep_config['number_of_filters'],\n            filter_size=(sweep_config['filter_size'], sweep_config['filter_size']),\n            stride=sweep_config['stride'],\n            padding=sweep_config['padding'],\n            max_pooling_size=(sweep_config['max_pooling_size'], sweep_config['max_pooling_size']),\n            n_neurons=sweep_config['n_neurons'],\n            n_classes=sweep_config['n_classes'],\n            conv_activation=activations[sweep_config['conv_activation']],\n            dense_activation=activations[sweep_config['dense_activation']],\n            dropout_rate=sweep_config['dropout_rate'],\n            use_batchnorm=sweep_config['use_batchnorm'],\n            factor=sweep_config['factor'],\n            dropout_organisation=sweep_config['dropout_organisation'],\n        ).to(device)\n        optimizer = Adam(model.parameters(), lr=sweep_config['learning_rate'])\n        criterion = nn.CrossEntropyLoss()\n        scaler = GradScaler()\n        best_val_accuracy = 0.0\n        for epoch in range(sweep_config[\"epochs\"]):\n            model.train()\n            train_loss = 0\n            correct = 0\n            total = 0\n            for x, y in train_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                optimizer.zero_grad()\n                \n                # Mixed precision training\n                with autocast():\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                \n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                train_loss += loss.item() * x.size(0)\n                correct += (pred.argmax(1) == y).sum().item()\n                total += y.size(0)\n                del x, y\n            train_accuracy = 100 * correct / total\n            avg_train_loss = train_loss / total\n            model.eval()\n            val_loss = 0\n            correct = 0\n            total = 0\n            with torch.no_grad():\n                for x, y in val_loader:\n                    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                    val_loss += loss.item() * x.size(0)\n                    correct += (pred.argmax(1) == y).sum().item()\n                    total += y.size(0)\n                    del x, y\n            val_accuracy = 100 * correct / total\n            avg_val_loss = val_loss / total\n            wandb.log({\n                'epoch': epoch+1,\n                'train_loss': avg_train_loss,\n                'train_accuracy': train_accuracy,\n                'val_loss': avg_val_loss,\n                'val_accuracy': val_accuracy\n            })\n            print(f\"Epoch [{epoch+1}/{sweep_config['epochs']}]\\nTrain Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\\nVal Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\\n\" + \"-\" * 50)\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n                torch.save(model.state_dict(), \"best_model_sweep.pth\")\n        print(f\"Best model saved to best_model.pth with val_accuracy={best_val_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:41:01.614276Z","iopub.execute_input":"2025-04-18T19:41:01.614535Z","iopub.status.idle":"2025-04-18T19:41:01.626252Z","shell.execute_reply.started":"2025-04-18T19:41:01.614518Z","shell.execute_reply":"2025-04-18T19:41:01.625664Z"}},"outputs":[],"execution_count":33},{"id":"W9uLeuMFUE6C","cell_type":"code","source":"if __name__ == \"__main__\":\n    wandb.login(key='f15dba29e56f32e9c31d598bce5bc7a3c76de62e')\n    sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment2\")\n    wandb.agent(sweep_id, function=wandb_sweep, count=20)  \n    wandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"W9uLeuMFUE6C","outputId":"de41ed92-93c5-49ca-c380-629fe783ab1b","executionInfo":{"status":"error","timestamp":1744956959600,"user_tz":-330,"elapsed":3096598,"user":{"displayName":"Ved","userId":"04705537962310421166"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:41:05.935457Z","iopub.execute_input":"2025-04-18T19:41:05.936080Z","iopub.status.idle":"2025-04-19T00:26:07.856759Z","shell.execute_reply.started":"2025-04-18T19:41:05.936054Z","shell.execute_reply":"2025-04-19T00:26:07.856166Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: 01tppmv2\nSweep URL: https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h5f54wn1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_194113-h5f54wn1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/h5f54wn1' target=\"_blank\">fiery-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/h5f54wn1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/h5f54wn1</a>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/1913325791.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_31/1913325791.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5]\nTrain Loss: 2.9309 | Train Accuracy: 10.70%\nVal Loss: 2.3053 | Val Accuracy: 9.95%\n--------------------------------------------------\nEpoch [2/5]\nTrain Loss: 2.3241 | Train Accuracy: 10.00%\nVal Loss: 2.3059 | Val Accuracy: 9.50%\n--------------------------------------------------\nEpoch [3/5]\nTrain Loss: 2.3102 | Train Accuracy: 10.29%\nVal Loss: 2.3055 | Val Accuracy: 9.95%\n--------------------------------------------------\nEpoch [4/5]\nTrain Loss: 2.3133 | Train Accuracy: 10.43%\nVal Loss: 2.3054 | Val Accuracy: 9.05%\n--------------------------------------------------\nEpoch [5/5]\nTrain Loss: 2.3086 | Train Accuracy: 10.05%\nVal Loss: 2.3020 | Val Accuracy: 10.65%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=10.65%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>█▁▄▅▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▅▃▅▁█</td></tr><tr><td>val_loss</td><td>▇█▇▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>10.05126</td></tr><tr><td>train_loss</td><td>2.30856</td></tr><tr><td>val_accuracy</td><td>10.65</td></tr><tr><td>val_loss</td><td>2.30201</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fiery-sweep-1</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/h5f54wn1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/h5f54wn1</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_194113-h5f54wn1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uicznblc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_194702-uicznblc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/uicznblc' target=\"_blank\">serene-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/uicznblc' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/uicznblc</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.0946 | Train Accuracy: 27.22%\nVal Loss: 2.1138 | Val Accuracy: 25.30%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.0024 | Train Accuracy: 32.93%\nVal Loss: 2.1167 | Val Accuracy: 24.45%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 1.9495 | Train Accuracy: 35.78%\nVal Loss: 2.1171 | Val Accuracy: 23.85%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 1.8986 | Train Accuracy: 38.37%\nVal Loss: 2.0334 | Val Accuracy: 29.55%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 1.8408 | Train Accuracy: 42.26%\nVal Loss: 2.0446 | Val Accuracy: 29.35%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 1.7701 | Train Accuracy: 46.38%\nVal Loss: 1.9916 | Val Accuracy: 30.15%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 1.6934 | Train Accuracy: 50.57%\nVal Loss: 1.9715 | Val Accuracy: 32.95%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 1.5901 | Train Accuracy: 57.54%\nVal Loss: 1.9622 | Val Accuracy: 33.40%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 1.4685 | Train Accuracy: 65.63%\nVal Loss: 1.9123 | Val Accuracy: 35.35%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 1.3245 | Train Accuracy: 75.07%\nVal Loss: 1.8917 | Val Accuracy: 37.25%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 1.1762 | Train Accuracy: 84.39%\nVal Loss: 1.8675 | Val Accuracy: 39.00%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 1.0228 | Train Accuracy: 91.64%\nVal Loss: 1.8928 | Val Accuracy: 37.15%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 0.8955 | Train Accuracy: 95.51%\nVal Loss: 1.8781 | Val Accuracy: 38.55%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 0.7772 | Train Accuracy: 97.97%\nVal Loss: 1.8608 | Val Accuracy: 40.95%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 0.6784 | Train Accuracy: 98.90%\nVal Loss: 1.8608 | Val Accuracy: 39.95%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=40.95%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▃▃▄▅▆▇▇███</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▆▆▅▄▃▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▂▁▁▃▃▄▅▅▆▆▇▆▇██</td></tr><tr><td>val_loss</td><td>███▆▆▅▄▄▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>98.89986</td></tr><tr><td>train_loss</td><td>0.67842</td></tr><tr><td>val_accuracy</td><td>39.95</td></tr><tr><td>val_loss</td><td>1.86076</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">serene-sweep-2</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/uicznblc' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/uicznblc</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_194702-uicznblc/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tq2ija08 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_202518-tq2ija08</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tq2ija08' target=\"_blank\">vivid-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tq2ija08' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tq2ija08</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.6043 | Train Accuracy: 9.76%\nVal Loss: 2.3023 | Val Accuracy: 9.60%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.3074 | Train Accuracy: 9.63%\nVal Loss: 2.2999 | Val Accuracy: 10.15%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.3001 | Train Accuracy: 11.66%\nVal Loss: 2.2994 | Val Accuracy: 12.50%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.2978 | Train Accuracy: 11.99%\nVal Loss: 2.2831 | Val Accuracy: 13.05%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.2979 | Train Accuracy: 11.86%\nVal Loss: 2.2915 | Val Accuracy: 11.70%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.2952 | Train Accuracy: 12.54%\nVal Loss: 2.2825 | Val Accuracy: 14.35%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.2980 | Train Accuracy: 12.49%\nVal Loss: 2.2922 | Val Accuracy: 12.25%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.3000 | Train Accuracy: 11.45%\nVal Loss: 2.3068 | Val Accuracy: 9.10%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 2.2985 | Train Accuracy: 12.03%\nVal Loss: 2.2825 | Val Accuracy: 12.35%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 2.3082 | Train Accuracy: 11.73%\nVal Loss: 2.2990 | Val Accuracy: 12.10%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 2.3050 | Train Accuracy: 11.99%\nVal Loss: 2.2921 | Val Accuracy: 12.90%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 2.3138 | Train Accuracy: 10.50%\nVal Loss: 2.2949 | Val Accuracy: 11.30%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 2.3032 | Train Accuracy: 12.05%\nVal Loss: 2.2863 | Val Accuracy: 11.70%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 2.2987 | Train Accuracy: 12.79%\nVal Loss: 2.3218 | Val Accuracy: 12.80%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 2.3051 | Train Accuracy: 12.18%\nVal Loss: 2.3046 | Val Accuracy: 9.10%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=14.35%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▆▆▆▇▇▅▆▆▆▃▆█▇</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▂▂▆▆▄█▅▁▅▅▆▄▄▆▁</td></tr><tr><td>val_loss</td><td>▅▄▄▁▃▁▃▅▁▄▃▃▂█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>12.17652</td></tr><tr><td>train_loss</td><td>2.30514</td></tr><tr><td>val_accuracy</td><td>9.1</td></tr><tr><td>val_loss</td><td>2.30464</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vivid-sweep-3</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tq2ija08' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tq2ija08</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_202518-tq2ija08/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2s3r5ivi with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_203520-2s3r5ivi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/2s3r5ivi' target=\"_blank\">toasty-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/2s3r5ivi' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/2s3r5ivi</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 12.5653 | Train Accuracy: 10.14%\nVal Loss: 2.3053 | Val Accuracy: 10.25%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.3141 | Train Accuracy: 10.09%\nVal Loss: 2.3065 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.3088 | Train Accuracy: 9.76%\nVal Loss: 2.3059 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.3046 | Train Accuracy: 9.33%\nVal Loss: 2.3045 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.3046 | Train Accuracy: 9.96%\nVal Loss: 2.3046 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.3041 | Train Accuracy: 10.24%\nVal Loss: 2.3050 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.3051 | Train Accuracy: 9.74%\nVal Loss: 2.3091 | Val Accuracy: 9.30%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.3186 | Train Accuracy: 10.10%\nVal Loss: 2.3066 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.36%\nVal Loss: 2.3068 | Val Accuracy: 9.05%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.96%\nVal Loss: 2.3036 | Val Accuracy: 11.00%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 2.3051 | Train Accuracy: 9.75%\nVal Loss: 2.3058 | Val Accuracy: 9.05%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 2.3052 | Train Accuracy: 9.58%\nVal Loss: 2.3037 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 2.3055 | Train Accuracy: 9.50%\nVal Loss: 2.3041 | Val Accuracy: 9.30%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 2.3042 | Train Accuracy: 10.01%\nVal Loss: 2.3032 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.78%\nVal Loss: 2.3058 | Val Accuracy: 9.05%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=11.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▇▇▄▁▆█▄▇▁▆▄▃▂▆▄</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▅▂▄▄▂▂▂▄▁█▁▂▂▄▁</td></tr><tr><td>val_loss</td><td>▃▅▄▃▃▃█▅▅▂▄▂▂▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>9.77622</td></tr><tr><td>train_loss</td><td>2.30492</td></tr><tr><td>val_accuracy</td><td>9.05</td></tr><tr><td>val_loss</td><td>2.30584</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">toasty-sweep-4</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/2s3r5ivi' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/2s3r5ivi</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_203520-2s3r5ivi/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ji9f0915 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_204827-ji9f0915</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/ji9f0915' target=\"_blank\">grateful-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/ji9f0915' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/ji9f0915</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 12.7028 | Train Accuracy: 9.63%\nVal Loss: 2.3040 | Val Accuracy: 9.65%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.3048 | Train Accuracy: 9.78%\nVal Loss: 2.3055 | Val Accuracy: 9.65%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.3048 | Train Accuracy: 9.63%\nVal Loss: 2.3054 | Val Accuracy: 9.25%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.3050 | Train Accuracy: 9.63%\nVal Loss: 2.3035 | Val Accuracy: 9.65%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.96%\nVal Loss: 2.3028 | Val Accuracy: 10.35%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.3047 | Train Accuracy: 9.74%\nVal Loss: 2.3068 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.3052 | Train Accuracy: 9.75%\nVal Loss: 2.3059 | Val Accuracy: 9.25%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.3048 | Train Accuracy: 9.81%\nVal Loss: 2.3067 | Val Accuracy: 9.25%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.78%\nVal Loss: 2.3028 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 2.3049 | Train Accuracy: 9.51%\nVal Loss: 2.3053 | Val Accuracy: 9.20%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 2.3044 | Train Accuracy: 10.14%\nVal Loss: 2.3024 | Val Accuracy: 11.30%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 2.3048 | Train Accuracy: 10.18%\nVal Loss: 2.3050 | Val Accuracy: 9.50%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 2.3050 | Train Accuracy: 9.16%\nVal Loss: 2.3091 | Val Accuracy: 9.25%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 2.3046 | Train Accuracy: 10.23%\nVal Loss: 2.3051 | Val Accuracy: 10.35%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 2.3051 | Train Accuracy: 9.85%\nVal Loss: 2.3060 | Val Accuracy: 9.50%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=11.30%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▄▅▄▄▆▅▅▅▅▃▇█▁█▆</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▂▂▁▂▅▁▁▁▃▁█▂▁▅▂</td></tr><tr><td>val_loss</td><td>▃▄▄▂▁▆▅▆▁▄▁▄█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>9.85123</td></tr><tr><td>train_loss</td><td>2.30509</td></tr><tr><td>val_accuracy</td><td>9.5</td></tr><tr><td>val_loss</td><td>2.30604</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">grateful-sweep-5</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/ji9f0915' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/ji9f0915</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_204827-ji9f0915/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tmaox4jo with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_210732-tmaox4jo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tmaox4jo' target=\"_blank\">twilight-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tmaox4jo' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tmaox4jo</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.2604 | Train Accuracy: 15.44%\nVal Loss: 2.2213 | Val Accuracy: 18.30%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.2362 | Train Accuracy: 17.51%\nVal Loss: 2.2272 | Val Accuracy: 17.15%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.2144 | Train Accuracy: 18.21%\nVal Loss: 2.2111 | Val Accuracy: 18.80%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.2045 | Train Accuracy: 18.74%\nVal Loss: 2.1948 | Val Accuracy: 18.50%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.2003 | Train Accuracy: 19.30%\nVal Loss: 2.2033 | Val Accuracy: 19.60%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.1917 | Train Accuracy: 19.58%\nVal Loss: 2.1810 | Val Accuracy: 21.15%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.1947 | Train Accuracy: 19.51%\nVal Loss: 2.1697 | Val Accuracy: 20.80%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.1806 | Train Accuracy: 20.63%\nVal Loss: 2.1799 | Val Accuracy: 19.40%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 2.1819 | Train Accuracy: 20.24%\nVal Loss: 2.1884 | Val Accuracy: 21.05%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 2.1811 | Train Accuracy: 20.37%\nVal Loss: 2.1887 | Val Accuracy: 19.80%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 2.1721 | Train Accuracy: 20.75%\nVal Loss: 2.1708 | Val Accuracy: 21.00%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 2.1661 | Train Accuracy: 21.09%\nVal Loss: 2.1801 | Val Accuracy: 20.95%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 2.1650 | Train Accuracy: 20.97%\nVal Loss: 2.1585 | Val Accuracy: 21.45%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 2.1663 | Train Accuracy: 20.72%\nVal Loss: 2.1567 | Val Accuracy: 21.50%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 2.1556 | Train Accuracy: 21.95%\nVal Loss: 2.1543 | Val Accuracy: 22.40%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=22.40%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▅▇▆▆▇▇▇▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▄▃▃▃▂▂▂▂▁</td></tr><tr><td>val_accuracy</td><td>▃▁▃▃▄▆▆▄▆▅▆▆▇▇█</td></tr><tr><td>val_loss</td><td>▇█▆▅▆▄▂▃▄▄▃▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>21.95274</td></tr><tr><td>train_loss</td><td>2.15562</td></tr><tr><td>val_accuracy</td><td>22.4</td></tr><tr><td>val_loss</td><td>2.15428</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">twilight-sweep-6</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tmaox4jo' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tmaox4jo</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_210732-tmaox4jo/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gsx6fzp8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_232511-gsx6fzp8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/gsx6fzp8' target=\"_blank\">laced-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/gsx6fzp8' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/gsx6fzp8</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.3104 | Train Accuracy: 10.13%\nVal Loss: 2.3064 | Val Accuracy: 9.50%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.3056 | Train Accuracy: 10.13%\nVal Loss: 2.3042 | Val Accuracy: 9.50%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.3036 | Train Accuracy: 10.28%\nVal Loss: 2.3034 | Val Accuracy: 9.50%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.3026 | Train Accuracy: 10.21%\nVal Loss: 2.3029 | Val Accuracy: 9.90%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.3016 | Train Accuracy: 10.55%\nVal Loss: 2.3033 | Val Accuracy: 9.30%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.3009 | Train Accuracy: 10.83%\nVal Loss: 2.3029 | Val Accuracy: 10.20%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.2990 | Train Accuracy: 11.29%\nVal Loss: 2.3018 | Val Accuracy: 10.50%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.2999 | Train Accuracy: 10.94%\nVal Loss: 2.3015 | Val Accuracy: 10.75%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 2.2989 | Train Accuracy: 11.26%\nVal Loss: 2.3011 | Val Accuracy: 11.20%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 2.2976 | Train Accuracy: 11.38%\nVal Loss: 2.3003 | Val Accuracy: 10.95%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 2.2970 | Train Accuracy: 11.83%\nVal Loss: 2.2996 | Val Accuracy: 11.00%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 2.2987 | Train Accuracy: 11.51%\nVal Loss: 2.3027 | Val Accuracy: 10.20%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 2.2965 | Train Accuracy: 12.19%\nVal Loss: 2.3019 | Val Accuracy: 10.95%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 2.2942 | Train Accuracy: 12.43%\nVal Loss: 2.2988 | Val Accuracy: 11.10%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 2.2958 | Train Accuracy: 12.31%\nVal Loss: 2.2985 | Val Accuracy: 11.15%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=11.20%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▃▅▃▄▅▆▅▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▃▂▂▃▂▁▂</td></tr><tr><td>val_accuracy</td><td>▂▂▂▃▁▄▅▆█▇▇▄▇██</td></tr><tr><td>val_loss</td><td>█▆▅▅▅▅▄▄▃▃▂▅▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>12.31404</td></tr><tr><td>train_loss</td><td>2.2958</td></tr><tr><td>val_accuracy</td><td>11.15</td></tr><tr><td>val_loss</td><td>2.29847</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">laced-sweep-7</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/gsx6fzp8' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/gsx6fzp8</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_232511-gsx6fzp8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zdlnthc2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_233609-zdlnthc2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zdlnthc2' target=\"_blank\">splendid-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zdlnthc2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zdlnthc2</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.1752 | Train Accuracy: 21.67%\nVal Loss: 2.1069 | Val Accuracy: 26.90%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.0681 | Train Accuracy: 28.25%\nVal Loss: 2.0766 | Val Accuracy: 26.95%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.0316 | Train Accuracy: 30.07%\nVal Loss: 2.0560 | Val Accuracy: 28.00%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.0040 | Train Accuracy: 31.28%\nVal Loss: 2.0282 | Val Accuracy: 30.00%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 1.9819 | Train Accuracy: 32.63%\nVal Loss: 2.0338 | Val Accuracy: 29.00%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 1.9604 | Train Accuracy: 33.82%\nVal Loss: 2.0162 | Val Accuracy: 29.65%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 1.9415 | Train Accuracy: 34.95%\nVal Loss: 1.9978 | Val Accuracy: 30.45%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 1.9204 | Train Accuracy: 36.44%\nVal Loss: 2.0024 | Val Accuracy: 30.40%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 1.9002 | Train Accuracy: 37.39%\nVal Loss: 1.9885 | Val Accuracy: 31.20%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 1.8800 | Train Accuracy: 38.33%\nVal Loss: 1.9739 | Val Accuracy: 32.20%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 1.8577 | Train Accuracy: 40.32%\nVal Loss: 1.9669 | Val Accuracy: 32.30%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 1.8355 | Train Accuracy: 41.14%\nVal Loss: 1.9544 | Val Accuracy: 33.40%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 1.8127 | Train Accuracy: 42.33%\nVal Loss: 1.9553 | Val Accuracy: 32.10%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 1.7908 | Train Accuracy: 43.68%\nVal Loss: 1.9452 | Val Accuracy: 33.05%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 1.7671 | Train Accuracy: 44.98%\nVal Loss: 1.9345 | Val Accuracy: 33.15%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=33.40%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▄▃▄▅▅▆▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▄▄▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>44.98062</td></tr><tr><td>train_loss</td><td>1.76707</td></tr><tr><td>val_accuracy</td><td>33.15</td></tr><tr><td>val_loss</td><td>1.93446</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">splendid-sweep-8</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zdlnthc2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zdlnthc2</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_233609-zdlnthc2/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k7rgznkh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_235743-k7rgznkh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/k7rgznkh' target=\"_blank\">dainty-sweep-9</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/k7rgznkh' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/k7rgznkh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/15]\nTrain Loss: 2.2789 | Train Accuracy: 14.68%\nVal Loss: 2.2353 | Val Accuracy: 18.70%\n--------------------------------------------------\nEpoch [2/15]\nTrain Loss: 2.1980 | Train Accuracy: 19.89%\nVal Loss: 2.1849 | Val Accuracy: 22.30%\n--------------------------------------------------\nEpoch [3/15]\nTrain Loss: 2.1390 | Train Accuracy: 23.15%\nVal Loss: 2.1414 | Val Accuracy: 23.35%\n--------------------------------------------------\nEpoch [4/15]\nTrain Loss: 2.1012 | Train Accuracy: 24.93%\nVal Loss: 2.1073 | Val Accuracy: 25.15%\n--------------------------------------------------\nEpoch [5/15]\nTrain Loss: 2.0718 | Train Accuracy: 26.62%\nVal Loss: 2.0980 | Val Accuracy: 25.25%\n--------------------------------------------------\nEpoch [6/15]\nTrain Loss: 2.0437 | Train Accuracy: 27.79%\nVal Loss: 2.0670 | Val Accuracy: 26.80%\n--------------------------------------------------\nEpoch [7/15]\nTrain Loss: 2.0243 | Train Accuracy: 28.12%\nVal Loss: 2.0583 | Val Accuracy: 26.05%\n--------------------------------------------------\nEpoch [8/15]\nTrain Loss: 2.0017 | Train Accuracy: 29.54%\nVal Loss: 2.0381 | Val Accuracy: 27.10%\n--------------------------------------------------\nEpoch [9/15]\nTrain Loss: 1.9904 | Train Accuracy: 29.73%\nVal Loss: 2.0373 | Val Accuracy: 26.90%\n--------------------------------------------------\nEpoch [10/15]\nTrain Loss: 1.9766 | Train Accuracy: 30.92%\nVal Loss: 2.0252 | Val Accuracy: 27.35%\n--------------------------------------------------\nEpoch [11/15]\nTrain Loss: 1.9577 | Train Accuracy: 31.13%\nVal Loss: 2.0226 | Val Accuracy: 25.55%\n--------------------------------------------------\nEpoch [12/15]\nTrain Loss: 1.9458 | Train Accuracy: 31.84%\nVal Loss: 2.0243 | Val Accuracy: 25.75%\n--------------------------------------------------\nEpoch [13/15]\nTrain Loss: 1.9389 | Train Accuracy: 31.67%\nVal Loss: 2.0172 | Val Accuracy: 25.60%\n--------------------------------------------------\nEpoch [14/15]\nTrain Loss: 1.9245 | Train Accuracy: 32.20%\nVal Loss: 2.0039 | Val Accuracy: 25.95%\n--------------------------------------------------\nEpoch [15/15]\nTrain Loss: 1.9126 | Train Accuracy: 33.07%\nVal Loss: 2.0374 | Val Accuracy: 22.00%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=27.35%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇█▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▂▂▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆█▇███▇▇▇▇▄</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_accuracy</td><td>33.06663</td></tr><tr><td>train_loss</td><td>1.91259</td></tr><tr><td>val_accuracy</td><td>22</td></tr><tr><td>val_loss</td><td>2.03737</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dainty-sweep-9</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/k7rgznkh' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/k7rgznkh</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_235743-k7rgznkh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tuuf4hu1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_001602-tuuf4hu1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tuuf4hu1' target=\"_blank\">comic-sweep-10</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tuuf4hu1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tuuf4hu1</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n    scaler.scale(loss).backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 490.12 MiB is free. Process 2641 has 14.26 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 208.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">comic-sweep-10</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tuuf4hu1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/tuuf4hu1</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_001602-tuuf4hu1/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run tuuf4hu1 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n    scaler.scale(loss).backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 490.12 MiB is free. Process 2641 has 14.26 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 208.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run tuuf4hu1 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scaler.scale(loss).backward()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 490.12 MiB is free. Process 2641 has 14.26 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 208.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n3cuftwq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_001821-n3cuftwq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/n3cuftwq' target=\"_blank\">sweet-sweep-11</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/n3cuftwq' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/n3cuftwq</a>"},"metadata":{}},{"name":"stdout","text":"Epoch [1/5]\nTrain Loss: 2.3483 | Train Accuracy: 10.71%\nVal Loss: 2.3121 | Val Accuracy: 8.45%\n--------------------------------------------------\nEpoch [2/5]\nTrain Loss: 2.3314 | Train Accuracy: 10.59%\nVal Loss: 2.3070 | Val Accuracy: 9.80%\n--------------------------------------------------\nEpoch [3/5]\nTrain Loss: 2.3214 | Train Accuracy: 10.79%\nVal Loss: 2.3062 | Val Accuracy: 9.30%\n--------------------------------------------------\nEpoch [4/5]\nTrain Loss: 2.3178 | Train Accuracy: 11.94%\nVal Loss: 2.3081 | Val Accuracy: 9.45%\n--------------------------------------------------\nEpoch [5/5]\nTrain Loss: 2.3162 | Train Accuracy: 11.54%\nVal Loss: 2.3081 | Val Accuracy: 9.05%\n--------------------------------------------------\nBest model saved to best_model.pth with val_accuracy=9.80%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▂▁▂█▆</td></tr><tr><td>train_loss</td><td>█▄▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁█▅▆▄</td></tr><tr><td>val_loss</td><td>█▂▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>11.53894</td></tr><tr><td>train_loss</td><td>2.31616</td></tr><tr><td>val_accuracy</td><td>9.05</td></tr><tr><td>val_loss</td><td>2.30805</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sweet-sweep-11</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/n3cuftwq' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/n3cuftwq</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_001821-n3cuftwq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oi3p97zm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002218-oi3p97zm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/oi3p97zm' target=\"_blank\">solar-sweep-12</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/oi3p97zm' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/oi3p97zm</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n    scaler.scale(loss).backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 62.12 MiB is free. Process 2641 has 14.68 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 33.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">solar-sweep-12</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/oi3p97zm' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/oi3p97zm</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002218-oi3p97zm/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run oi3p97zm errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n    scaler.scale(loss).backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 62.12 MiB is free. Process 2641 has 14.68 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 33.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oi3p97zm errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 70, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scaler.scale(loss).backward()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 62.12 MiB is free. Process 2641 has 14.68 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 33.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u0gaaysk with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002239-u0gaaysk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u0gaaysk' target=\"_blank\">charmed-sweep-13</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u0gaaysk' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u0gaaysk</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 548.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2641 has 14.69 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 30.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">charmed-sweep-13</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u0gaaysk' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u0gaaysk</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002239-u0gaaysk/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run u0gaaysk errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 548.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2641 has 14.69 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 30.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run u0gaaysk errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ).to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m       ^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 548.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2641 has 14.69 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 30.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dl7meuqk with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002325-dl7meuqk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/dl7meuqk' target=\"_blank\">chocolate-sweep-14</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/dl7meuqk' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/dl7meuqk</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 67, in wandb_sweep\n    pred = model(x)\n           ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1099850962.py\", line 93, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 432, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2380, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 37.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">chocolate-sweep-14</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/dl7meuqk' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/dl7meuqk</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002325-dl7meuqk/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run dl7meuqk errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 67, in wandb_sweep\n    pred = model(x)\n           ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1099850962.py\", line 93, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 432, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2380, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 37.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run dl7meuqk errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 67, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     pred = model(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1099850962.py\", line 93, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     input = module(input)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 432, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.silu(input, inplace=self.inplace)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2380, in silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch._C._nn.silu(input)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 37.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kvfu7qsx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002346-kvfu7qsx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/kvfu7qsx' target=\"_blank\">swept-sweep-15</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/kvfu7qsx' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/kvfu7qsx</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">swept-sweep-15</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/kvfu7qsx' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/kvfu7qsx</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002346-kvfu7qsx/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run kvfu7qsx errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run kvfu7qsx errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2641 has 14.73 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u2hr5wmc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002407-u2hr5wmc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u2hr5wmc' target=\"_blank\">graceful-sweep-16</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u2hr5wmc' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u2hr5wmc</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 36.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">graceful-sweep-16</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u2hr5wmc' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/u2hr5wmc</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002407-u2hr5wmc/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run u2hr5wmc errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 36.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run u2hr5wmc errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 36.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1lcghy8k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002428-1lcghy8k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/1lcghy8k' target=\"_blank\">young-sweep-17</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/1lcghy8k' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/1lcghy8k</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">young-sweep-17</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/1lcghy8k' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/1lcghy8k</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002428-1lcghy8k/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 1lcghy8k errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1lcghy8k errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 62, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 23u16tuq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002449-23u16tuq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/23u16tuq' target=\"_blank\">elated-sweep-18</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/23u16tuq' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/23u16tuq</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">elated-sweep-18</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/23u16tuq' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/23u16tuq</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002449-23u16tuq/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 23u16tuq errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 23u16tuq errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ).to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m       ^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zoebx0el with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: tanh\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: sigmoid\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002510-zoebx0el</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zoebx0el' target=\"_blank\">solar-sweep-19</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zoebx0el' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zoebx0el</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">solar-sweep-19</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zoebx0el' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/zoebx0el</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002510-zoebx0el/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run zoebx0el errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run zoebx0el errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ).to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m       ^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 34.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5aee1nm1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: relu6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_organisation: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_pooling_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_classes: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_augmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batchnorm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_002556-5aee1nm1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/5aee1nm1' target=\"_blank\">cosmic-sweep-20</a></strong> to <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/sweeps/01tppmv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/5aee1nm1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/5aee1nm1</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 33.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cosmic-sweep-20</strong> at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/5aee1nm1' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2/runs/5aee1nm1</a><br> View project at: <a href='https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2' target=\"_blank\">https://wandb.ai/ma23c047-indian-institute-of-technology-madras/DA6401_Assignment2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_002556-5aee1nm1/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 5aee1nm1 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n    ).to(device)\n      ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 33.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5aee1nm1 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1913325791.py\", line 51, in wandb_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ).to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m       ^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 2641 has 14.74 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 33.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","output_type":"stream"}],"execution_count":34},{"id":"lZvoINoGULHt","cell_type":"code","source":"","metadata":{"id":"lZvoINoGULHt"},"outputs":[],"execution_count":null}]}